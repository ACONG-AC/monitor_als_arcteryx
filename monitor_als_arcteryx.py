#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ALS.com Arc'teryx ç›‘æ§
- ä¸Šæ–°ï¼ˆæ–°å•†å“/æ–°å˜ä½“ï¼‰
- ä»·æ ¼å˜åŒ–
- ä»…æé†’â€œç¼ºè´§â†’åˆ°è´§â€
- åº“å­˜æ•°é‡å¢åŠ ï¼ˆæŒ‰å°ºç å¯¹æ¯”æ•°é‡ï¼›è‹¥æ— ç²¾ç¡®æ•°é‡åˆ™ç”¨0/1è¿‘ä¼¼ï¼‰

é€šçŸ¥æ ¼å¼ï¼š
â€¢ åç§°ï¼š{title}
â€¢ è´§å·ï¼š{sku}
â€¢ é¢œè‰²ï¼š{color}
â€¢ ä»·æ ¼ï¼š{currency}{price}
ğŸ§¾ åº“å­˜ä¿¡æ¯ï¼š{size1:qty1, size2:qty2, ...}
{url}

Env:
  DISCORD_WEBHOOK_URL   å¿…å¡«ï¼šDiscord Webhook
  ALWAYS_NOTIFY=1       å¯é€‰ï¼šå³ä½¿æ— å˜åŒ–ä¹Ÿå‘ä¸€æ¡ï¼ˆè¿é€šæ€§æµ‹è¯•ï¼‰
  HEADLESS=0            å¯é€‰ï¼šæœ¬åœ°è°ƒè¯•è®¾ä¸º0ï¼ŒCIé»˜è®¤1
  KEYWORD_FILTER        å¯é€‰ï¼šä»…ç›‘æ§åŒ…å«è¯¥å…³é”®è¯çš„æ ‡é¢˜
"""

import json
import os
import re
import sys
import time
import math
import random
import shutil
from datetime import datetime, timezone
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Dict, Any, List, Tuple

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout

COLLECTION_URL = "https://www.als.com/arc-teryx"
SNAPSHOT_PATH = Path("snapshot.json")
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

# --------------------------
# Utilities
# --------------------------

def jdump(obj: Any, path: Path):
    """Atomic write to avoid half-written or empty JSON."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with NamedTemporaryFile('w', delete=False, encoding='utf-8', dir=str(path.parent)) as tmp:
        json.dump(obj, tmp, ensure_ascii=False, indent=2)
        tmp.flush()
        os.fsync(tmp.fileno())
        tmp_name = tmp.name
    try:
        shutil.move(tmp_name, path)
    finally:
        try:
            os.unlink(tmp_name)
        except Exception:
            pass


def jload(path: Path) -> Dict[str, Any]:
    if not path.exists():
        print(f"[snapshot] {path} not found.")
        return {}
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        print(f"[snapshot] loaded {len(data)} items from {path}.")
        return data
    except Exception as e:
        print(f"[snapshot] failed to parse {path}: {e}")
        return {}


def safe_sleep(a=0.3, b=0.9):
    time.sleep(random.uniform(a, b))


def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def norm_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def normalize_key(title: str, sku: str, color: str, url: str) -> str:
    """ä¼˜å…ˆç”¨ sku+colorï¼Œå…¶æ¬¡ title+colorï¼Œæœ€åå›é€€ url æ®µ"""
    if sku and color:
        return f"{sku.lower()}::{color.lower()}"
    if title and color:
        return f"{title.lower()}::{color.lower()}"
    m = re.search(r"/([^/]+)/p(?:$|\?)", url)
    slug = m.group(1).lower() if m else re.sub(r"[^a-z0-9]+", "-", (title or url).lower())
    return f"{slug}::{color.lower() if color else 'na'}"


def money_from_text(txt: str):
    """
    æŠ½å–è´§å¸ç¬¦å·ä¸é‡‘é¢ï¼Œä¾‹å¦‚ '$ 360.00' æˆ– 'CA$ 360'ã€‚
    è¿”å› (currency_symbol, price_float)ï¼›è‹¥å¤±è´¥ price=nan, symbol=''
    """
    if not txt:
        return "", math.nan
    # å¸¸è§ï¼š'$360.00' 'CA$ 360' 'US$ 200'
    m = re.search(r"([A-Z]{2}\$|\$|C\$|CA\$|US\$|â‚¬|Â£|Â¥)\s*([0-9]+(?:\.[0-9]{2})?)", txt.replace(",", ""))
    if m:
        return m.group(1), float(m.group(2))
    # é€€è·¯ï¼šåªæ‰¾é‡‘é¢
    m = re.search(r"([0-9]+(?:\.[0-9]{2})?)", txt.replace(",", ""))
    if m:
        return "", float(m.group(1))
    return "", math.nan


# --------------------------
# Scraper
# --------------------------

def extract_collection_links(page) -> List[str]:
    """æ”¶é›†é›†åˆé¡µä¸Šçš„ PDP é“¾æ¥"""
    anchors = page.locator("a[href*='/arcteryx-'][href*='/p']")
    hrefs = anchors.evaluate_all("els => els.map(e => e.href)")
    uniq = []
    for h in hrefs:
        if "als.com" in h:
            h = h.split("#")[0]
            if h not in uniq:
                uniq.append(h)
    return uniq


def extract_sku(page) -> str:
    """
    è§£æè´§å·ï¼ˆSKUï¼‰ã€‚å¸¸è§ä½ç½®ï¼š
    - æ˜æ–‡ 'SKU:'ã€'Style #'ã€'Model #'
    - meta/ld+json ä¸­çš„ 'sku'
    - ä»¥ X0000... å½¢å¼
    """
    # 1) DOM æ–‡æœ¬
    try:
        txt = page.locator("body").inner_text()
        # ä¼˜å…ˆ X000... æ ·å¼
        m = re.search(r"(X\d{9,12})", txt)
        if m:
            return m.group(1).strip()
        # é€šç”¨ SKU/Style/Model
        m = re.search(r"(?:SKU|Style|Model)\s*[:#]\s*([A-Za-z0-9\-]+)", txt, re.I)
        if m:
            return m.group(1).strip()
    except Exception:
        pass
    # 2) å…ƒæ•°æ®
    try:
        metas = page.locator("script[type='application/ld+json']")
        for i in range(metas.count()):
            raw = metas.nth(i).inner_text()
            for obj in json.loads(raw if raw.strip().startswith("{") else "{}"),:
                if isinstance(obj, dict):
                    sku = obj.get("sku") or ""
                    if sku:
                        return str(sku).strip()
    except Exception:
        pass
    return ""


def extract_color(page) -> str:
    """
    è§£æå½“å‰é€‰ä¸­é¢œè‰²ã€‚å¸¸è§ï¼š
    - 'Color: Trail Magic'
    - é¢œè‰²é€‰æ‹©å™¨çš„ aria-pressed / selected æ–‡æœ¬
    """
    # 1) Label å½¢å¼
    try:
        # æŸ¥å¸¦ "Color" çš„æ–‡æœ¬
        matches = page.locator("text=/Color\\s*:/i")
        if matches.count():
            # å–åŒ…å«å†’å·çš„è¿™ä¸€è¡Œ
            line = matches.first.evaluate("el => el.parentElement ? el.parentElement.innerText : el.innerText")
            if line:
                m = re.search(r"Color\s*:\s*(.+)", line, re.I)
                if m:
                    return norm_spaces(m.group(1))
    except Exception:
        pass
    # 2) é¢œè‰²æŒ‰é’®ï¼ˆé€‰ä¸­é¡¹ï¼‰
    try:
        selected = page.locator("[aria-pressed='true'], [aria-selected='true']")
        for i in range(min(selected.count(), 10)):
            t = norm_spaces(selected.nth(i).inner_text())
            if t and len(t) <= 40 and not re.search(r"(Add to cart|Add to bag)", t, re.I):
                return t
    except Exception:
        pass
    # 3) æ ‡é¢˜ä¸­å¸¦é¢œè‰²
    try:
        title = page.locator("h1").first.inner_text() if page.locator("h1").count() else ""
        # ç»éªŒï¼šé¢œè‰²æœ‰æ—¶åœ¨æ ‡é¢˜æœ«å°¾æ‹¬å·é‡Œ
        m = re.search(r"\(([^()]+)\)$", title)
        if m:
            return norm_spaces(m.group(1))
    except Exception:
        pass
    return ""


def extract_price(page) -> Tuple[str, float]:
    """è§£æè´§å¸ä¸ä»·æ ¼"""
    # å°è¯•å¤šä¸ªé€‰æ‹©å™¨
    candidates = [
        "[class*='price']",
        "[data-test*='price']",
        "div:has-text('$')",
        "div:has-text('US$'), div:has-text('CA$'), div:has-text('C$'), div:has-text('Â¥'), div:has-text('â‚¬'), div:has-text('Â£')",
        "body",
    ]
    for sel in candidates:
        try:
            if page.locator(sel).count():
                txt = page.locator(sel).first.inner_text()
                cur, pr = money_from_text(txt)
                if not math.isnan(pr):
                    return cur, pr
        except Exception:
            continue
    return "", math.nan


def extract_sizes_with_qty(page) -> Dict[str, int]:
    """
    è¿”å› dict: {size_text: qty_int}
    è§£æé¡ºåºï¼š
    1) å¸¦æ•°é‡çš„æ•°æ®å±æ€§ï¼šdata-available-qty / data-inventory / data-qty / data-stock
    2) å†…åµŒ JSONï¼ˆvariants / optionsï¼‰
    3) å›é€€ï¼šæŒ‰é’®å¯ç‚¹=1ï¼Œä¸å¯ç‚¹=0
    """
    sizes: Dict[str, int] = {}

    # 1) æŒ‰é’®/é€‰é¡¹å¸¦æ•°æ®å±æ€§
    try:
        btns = page.locator("button, [role='option'], [data-size]")
        for i in range(min(200, btns.count())):
            el = btns.nth(i)
            label = norm_spaces(el.inner_text())
            if not label or len(label) > 10:  # è¿‡æ»¤éå°ºç 
                continue
            if not re.fullmatch(r"(XXS|XS|S|M|L|XL|XXL|XXXL|[\d]{1,2})", label, re.I):
                continue
            qty_attr = None
            for attr in ("data-available-qty", "data-inventory", "data-qty", "data-stock", "data-quantity"):
                v = el.get_attribute(attr)
                if v and re.fullmatch(r"\d+", v.strip()):
                    qty_attr = int(v.strip())
                    break
            if qty_attr is not None:
                sizes[label.upper()] = max(0, qty_attr)
    except Exception:
        pass

    # 2) å†…åµŒ JSONï¼ˆæœ‰æ—¶é¡µé¢ä¼šæœ‰ variants åˆ—è¡¨ï¼‰
    if not sizes:
        try:
            scripts = page.locator("script")
            for i in range(min(20, scripts.count())):
                raw = scripts.nth(i).inner_text()
                if not raw or ("variant" not in raw.lower() and "inventory" not in raw.lower()):
                    continue
                # ç²—æš´æ‰¾å‡ºç±»ä¼¼ ... "size":"XL","inventory_quantity":3 ...
                for m in re.finditer(r'"size"\s*:\s*"(?P<size>[^"]+?)"[^}]*?"inventory[^"]*?"\s*:\s*(?P<qty>-?\d+)', raw, re.I | re.S):
                    size = m.group("size").strip().upper()
                    qty = int(m.group("qty"))
                    sizes[size] = max(0, qty)
        except Exception:
            pass

    # 3) å›é€€ï¼šå¯ç‚¹=1ï¼Œä¸å¯ç‚¹=0ï¼ˆä¿è¯èƒ½åšâ€œç¼ºè´§â†’åˆ°è´§/æ•°é‡å¢åŠ â€çš„åˆ¤æ–­ï¼‰
    if not sizes:
        try:
            candidates = page.locator(
                "button:has-text('XXS'), button:has-text('XS'), button:has-text('S'), "
                "button:has-text('M'), button:has-text('L'), button:has-text('XL'), "
                "button:has-text('XXL'), button:has-text('XXXL')"
            )
            for i in range(candidates.count()):
                el = candidates.nth(i)
                label = norm_spaces(el.inner_text()).upper()
                if not label:
                    continue
                disabled = el.get_attribute("disabled")
                aria = el.get_attribute("aria-disabled")
                cls = (el.get_attribute("class") or "")
                sizes[label] = 0 if (disabled is not None or aria in ("true", "disabled") or "disabled" in cls) else 1
        except Exception:
            pass

    return sizes


def parse_product_detail(page) -> Dict[str, Any]:
    """è§£æ PDP æ‰€éœ€å­—æ®µ"""
    data = {
        "title": "",
        "sku": "",
        "color": "",
        "currency": "",
        "price": math.nan,
        "sizes": {},       # {size: qty_int}
        "in_stock": False, # æ˜¯å¦æ•´ä½“å¯ä¹°ï¼ˆä»»ä¸€å°ºç  qty>0 å³ Trueï¼‰
    }

    # æ ‡é¢˜
    try:
        if page.locator("h1").count():
            data["title"] = norm_spaces(page.locator("h1").first.inner_text())
        elif page.locator("title").count():
            data["title"] = norm_spaces(page.locator("title").first.inner_text())
    except Exception:
        pass

    # è´§å·
    try:
        data["sku"] = extract_sku(page)
    except Exception:
        pass

    # é¢œè‰²
    try:
        data["color"] = extract_color(page)
    except Exception:
        pass

    # ä»·æ ¼
    try:
        cur, pr = extract_price(page)
        data["currency"] = cur
        data["price"] = pr
    except Exception:
        pass

    # å°ºç ä¸æ•°é‡
    try:
        sizes = extract_sizes_with_qty(page)
        data["sizes"] = sizes
        data["in_stock"] = any(qty > 0 for qty in sizes.values()) if sizes else False
    except Exception:
        pass

    return data


def scrape_all_products(headless=True, timeout_ms=15000) -> Dict[str, Any]:
    """éå†é›†åˆé¡µ â†’ é€ä¸ª PDP è§£æ â†’ è¿”å›ä»¥ variant key ä¸ºé”®çš„ dict"""
    result: Dict[str, Any] = {}
    keyword = os.environ.get("KEYWORD_FILTER", "").strip().lower()

    with sync_playwright() as pw:
        browser = pw.chromium.launch(headless=headless)
        ctx = browser.new_context(user_agent=USER_AGENT, locale="en-US")
        page = ctx.new_page()

        page_idx = 1
        empty_hits = 0
        seen_urls = set()

        while True:
            url = COLLECTION_URL if page_idx == 1 else f"{COLLECTION_URL}?page={page_idx}"
            try:
                page.goto(url, timeout=timeout_ms)
                page.wait_for_load_state("domcontentloaded", timeout=timeout_ms)
            except PWTimeout:
                print(f"[page] timeout loading {url}")
                empty_hits += 1
                if empty_hits >= 2:
                    break
                page_idx += 1
                continue

            links = extract_collection_links(page)
            print(f"[collection] page {page_idx} links: {len(links)}")

            if not links:
                empty_hits += 1
                if empty_hits >= 2:
                    break
                page_idx += 1
                continue

            empty_hits = 0
            for href in links:
                if href in seen_urls:
                    continue
                seen_urls.add(href)
                safe_sleep(0.4, 1.0)

                ok = False
                for attempt in range(3):
                    try:
                        page.goto(href, timeout=timeout_ms)
                        page.wait_for_load_state("domcontentloaded", timeout=timeout_ms)
                        safe_sleep(0.2, 0.6)
                        pdata = parse_product_detail(page)
                        title = pdata.get("title", "")
                        color = pdata.get("color", "")
                        sku = pdata.get("sku", "")
                        if keyword and keyword not in (title or "").lower():
                            ok = True
                            break
                        if title:
                            key = normalize_key(title, sku, color, href)
                            pdata.update({"url": href, "last_seen": now_iso(), "key": key})
                            result[key] = pdata
                            ok = True
                            break
                    except Exception as e:
                        print(f"[detail] error {href}: {e}")
                        safe_sleep(0.7, 1.5)
                if not ok:
                    # è®°å½•æœ€å°‘ä¿¡æ¯ä»¥å…ä¸¢å¤±
                    key = normalize_key("", "", "", href)
                    result[key] = {
                        "title": "",
                        "sku": "",
                        "color": "",
                        "currency": "",
                        "price": math.nan,
                        "sizes": {},
                        "in_stock": False,
                        "url": href,
                        "last_seen": now_iso(),
                        "key": key,
                        "note": "parse_failed",
                    }
            page_idx += 1

        ctx.close()
        browser.close()

    return result


# --------------------------
# Diff & Notification
# --------------------------

def compute_diff(old: Dict[str, Any], new: Dict[str, Any]):
    """
    è¿”å›ï¼š
      new_items:        æ–°å•†å“/æ–°å˜ä½“
      price_changes:    ä»·æ ¼å˜åŒ–
      restocks:         ç¼ºè´§â†’åˆ°è´§ï¼ˆold.in_stock=False & new.in_stock=Trueï¼‰
      stock_increases:  åº“å­˜æ•°é‡å¢åŠ ï¼ˆæŒ‰å°ºç å¯¹æ¯”ï¼›è‹¥è§£æä¸åˆ°æ•°é‡ï¼Œç”¨0/1ï¼‰
                         å…ƒç´ ç»“æ„ï¼š[(key, old, new, increased_sizes_dict)]
    """
    new_items = []
    price_changes = []
    restocks = []
    stock_increases = []

    old_keys = set(old.keys())
    new_keys = set(new.keys())

    # ä¸Šæ–°ï¼ˆå«æ–°å˜ä½“ï¼‰
    for k in sorted(new_keys - old_keys):
        new_items.append((k, None, new[k]))

    # äº¤é›†å¯¹æ¯”
    for k in sorted(new_keys & old_keys):
        o = old[k] or {}
        n = new[k] or {}

        # ä»·æ ¼å˜åŒ–
        op, np = o.get("price"), n.get("price")
        if (isinstance(op, (int, float)) and isinstance(np, (int, float))
                and not math.isnan(op) and not math.isnan(np) and abs(op - np) >= 0.01):
            price_changes.append((k, o, n))

        # ç¼ºè´§â†’åˆ°è´§ï¼ˆä»…æé†’è¿™ä¸€æ–¹å‘ï¼‰
        if (not o.get("in_stock", False)) and n.get("in_stock", False):
            restocks.append((k, o, n))

        # åº“å­˜æ•°é‡å¢åŠ ï¼ˆé€å°ºç ï¼‰
        increased: Dict[str, int] = {}
        osizes: Dict[str, int] = o.get("sizes") or {}
        nsizes: Dict[str, int] = n.get("sizes") or {}
        for size, nqty in nsizes.items():
            oqty = osizes.get(size, 0)
            try:
                if int(nqty) > int(oqty):
                    increased[size] = int(nqty)
            except Exception:
                # éæ³•å€¼æŒ‰0/1é€»è¾‘
                if (nqty and not oqty):
                    increased[size] = 1
        if increased:
            stock_increases.append((k, o, n, increased))

    return {
        "new_items": new_items,
        "price_changes": price_changes,
        "restocks": restocks,
        "stock_increases": stock_increases,
    }


def _fmt_currency_price(currency: str, price: float) -> str:
    if isinstance(price, (int, float)) and not math.isnan(price):
        cur = (currency or "").strip()
        # ç»Ÿä¸€å»æ‰å¤šä½™ç©ºæ ¼ï¼š'CA$ ' â†’ 'CA$ '
        return f"{cur} {price:.2f}".strip()
    return "N/A"


def _fmt_sizes_line(sizes: Dict[str, int], only_keys: List[str] = None, limit: int = 8) -> str:
    items = []
    if only_keys:
        for k in only_keys:
            if k in sizes:
                items.append(f"{k}:{sizes[k]}")
    else:
        # ä»…å±•ç¤ºæœ‰åº“å­˜ï¼ˆ>0ï¼‰çš„å°ºç ï¼Œæœ€å¤š limit ä¸ª
        for k, v in sizes.items():
            if v and v > 0:
                items.append(f"{k}:{v}")
                if len(items) >= limit:
                    break
    return "ï¼Œ".join(items) if items else "æ— "


def format_discord_message(diffs) -> Dict[str, Any]:
    """æŒ‰æŒ‡å®šæ ¼å¼ç»„ç»‡ä¸º Discord åµŒå…¥æ¶ˆæ¯"""
    lines: List[str] = []

    def block(n: Dict[str, Any], title: str):
        nm = n.get("title") or "-"
        sku = n.get("sku") or "-"
        color = n.get("color") or "-"
        price = _fmt_currency_price(n.get("currency", ""), n.get("price"))
        sizes = n.get("sizes") or {}
        # æŒ‰ä½ çš„ç¤ºä¾‹æ ¼å¼è¾“å‡º
        lines.append(f"â€¢ åç§°ï¼š{nm}")
        lines.append(f"â€¢ è´§å·ï¼š{sku}")
        lines.append(f"â€¢ é¢œè‰²ï¼š{color}")
        lines.append(f"â€¢ ä»·æ ¼ï¼š{price}")
        lines.append(f"ğŸ§¾ åº“å­˜ä¿¡æ¯ï¼š{_fmt_sizes_line(sizes)}")
        lines.append(f"{n.get('url')}")
        lines.append("")  # ç©ºè¡Œåˆ†éš”

    # ä¸Šæ–°
    if diffs["new_items"]:
        lines.append("**ä¸Šæ–°ï¼ˆæ–°å•†å“/æ–°å˜ä½“ï¼‰**")
        for k, _, n in diffs["new_items"][:20]:
            block(n, "ä¸Šæ–°")

    # ä»·æ ¼å˜åŒ–
    if diffs["price_changes"]:
        lines.append("**ä»·æ ¼å˜åŒ–**")
        for k, o, n in diffs["price_changes"][:20]:
            block(n, "ä»·æ ¼å˜åŒ–")

    # ç¼ºè´§â†’åˆ°è´§
    if diffs["restocks"]:
        lines.append("**ç¼ºè´§ â†’ åˆ°è´§**")
        for k, o, n in diffs["restocks"][:20]:
            block(n, "åˆ°è´§")

    # åº“å­˜æ•°é‡å¢åŠ ï¼ˆä»…å±•ç¤ºå¢åŠ çš„å°ºç ï¼‰
    if diffs["stock_increases"]:
        lines.append("**åº“å­˜æ•°é‡å¢åŠ **")
        for k, o, n, inc in diffs["stock_increases"][:20]:
            nm = n.get("title") or "-"
            sku = n.get("sku") or "-"
            color = n.get("color") or "-"
            price = _fmt_currency_price(n.get("currency", ""), n.get("price"))
            sizes = n.get("sizes") or {}
            inc_keys = list(inc.keys())
            lines.append(f"â€¢ åç§°ï¼š{nm}")
            lines.append(f"â€¢ è´§å·ï¼š{sku}")
            lines.append(f"â€¢ é¢œè‰²ï¼š{color}")
            lines.append(f"â€¢ ä»·æ ¼ï¼š{price}")
            lines.append(f"ğŸ§¾ åº“å­˜ä¿¡æ¯ï¼š{_fmt_sizes_line(sizes, only_keys=inc_keys)}")
            lines.append(f"{n.get('url')}")
            lines.append("")

    content = "\n".join(lines) if lines else "æœ¬æ¬¡æ‰«ææœªå‘ç°å˜åŒ–ã€‚"

    payload = {
        "content": None,
        "embeds": [{
            "title": "Al's | Arc'teryx ç›‘æ§ç»“æœ",
            "description": content[:4000],  # ä¿é™©èµ·è§é™åˆ¶æè¿°é•¿åº¦
            "timestamp": datetime.utcnow().isoformat(),
            "color": 0x00AAFF,
            "footer": {"text": "als.com ä»·æ ¼/ä¸Šæ–°/åº“å­˜ç›‘æ§"},
        }]
    }
    return payload


def send_discord(payload: dict) -> None:
    """
    Discord Webhook é€šçŸ¥ï¼šä»…å¿…è¦è¯·æ±‚å¤´ + é‡è¯•
    ï¼ˆå»æ‰ Origin/Refererï¼Œé¿å… 50067 Invalid request originï¼‰
    """
    import urllib.request
    import urllib.error

    webhook = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
    if not webhook:
        print("WARN: DISCORD_WEBHOOK_URL æœªé…ç½®ï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return

    webhook = webhook.replace("discordapp.com", "discord.com")
    if "?" not in webhook:
        webhook = webhook + "?wait=true"

    data = json.dumps(payload).encode("utf-8")
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
    }

    for attempt in range(4):
        req = urllib.request.Request(webhook, data=data, headers=headers, method="POST")
        try:
            with urllib.request.urlopen(req, timeout=20) as resp:
                body = resp.read().decode("utf-8", "ignore")
                print(f"Discord sent OK: {resp.status} {body[:200]}")
                return
        except urllib.error.HTTPError as e:
            body = e.read().decode("utf-8", "ignore")
            print(f"Discord HTTPError: {e.code} {body[:300]}")
            if e.code in (429, 403, 502, 503) and attempt < 3:
                wait = max(2 ** attempt, float(e.headers.get("Retry-After", "0") or 0))
                print(f"ç­‰å¾… {wait} ç§’åé‡è¯•...")
                time.sleep(wait)
                continue
            print("æ”¾å¼ƒé‡è¯•ã€‚")
            return
        except Exception as ex:
            print(f"Discord error: {repr(ex)}")
            if attempt < 3:
                wait = 2 ** attempt
                print(f"ç­‰å¾… {wait} ç§’åé‡è¯•...")
                time.sleep(wait)
                continue
            return


# --------------------------
# Main
# --------------------------

def main():
    print(f"CWD={os.getcwd()}  SNAPSHOT_PATH={SNAPSHOT_PATH.resolve()}")
    headless = os.environ.get("HEADLESS", "1") != "0"

    old = jload(SNAPSHOT_PATH)
    print(f"Loaded {len(old)} items from snapshot.")

    new = scrape_all_products(headless=headless)
    print(f"Scraped {len(new)} items from website.")

    diffs = compute_diff(old, new)
    print("Diff summary:",
          f"new={len(diffs['new_items'])},",
          f"price={len(diffs['price_changes'])},",
          f"restock={len(diffs['restocks'])},",
          f"stock_inc={len(diffs['stock_increases'])}")

    jdump(new, SNAPSHOT_PATH)

    if (sum(len(v) for v in diffs.values()) > 0) or os.environ.get("ALWAYS_NOTIFY", "0") == "1":
        payload = format_discord_message(diffs)
        send_discord(payload)
    else:
        print("No diff; not notifying.")

    return 0


if __name__ == "__main__":
    sys.exit(main())
